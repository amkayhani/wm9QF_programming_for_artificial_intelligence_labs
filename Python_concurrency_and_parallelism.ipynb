{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amkayhani/wm9QF_programming_for_artificial_intelligence_labs/blob/main/Python_concurrency_and_parallelism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhpRpPOVVe2R"
      },
      "source": [
        "# Python Advanced: Concurrency and Parallelism\n",
        "\n",
        "This notebook mirrors the LiaScript lesson and keeps the examples runnable with extra guidance for beginners. You will learn when to use threading, multiprocessing, and asyncio to keep programs responsive and fast.\n",
        "\n",
        "**Prerequisites:** comfortable with Python functions, loops, and basic exception handling.\n",
        "\n",
        "**You will be able to:**\n",
        "- Distinguish concurrency, parallelism, and asynchronous programming\n",
        "- Choose between threading, multiprocessing, and asyncio for different workloads\n",
        "- Spot issues like race conditions and deadlocks\n",
        "- Apply these ideas to web scraping, data processing, and API-heavy code"
      ],
      "id": "MhpRpPOVVe2R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVw_g4FzVe2U"
      },
      "source": [
        "## Concurrency vs. Parallelism\n",
        "\n",
        "- **Concurrency**: dealing with many tasks at once by switching between them (great when tasks wait for I/O).\n",
        "- **Parallelism**: doing multiple tasks at the exact same time on different CPU cores.\n",
        "- Python gives you three main tools:\n",
        "  1. **Threading**: many threads in one process (best for I/O-bound work).\n",
        "  2. **Multiprocessing**: many processes with separate memory (best for CPU-bound work).\n",
        "  3. **Asyncio**: cooperative multitasking with `async`/`await` (great for lots of network-bound tasks)."
      ],
      "id": "VVw_g4FzVe2U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL1f2h9zVe2V"
      },
      "source": [
        "## The Global Interpreter Lock (GIL)\n",
        "\n",
        "The GIL is a lock in CPython that allows only one thread to run Python bytecode at a time. Effects:\n",
        "- Threading can speed up I/O-bound work because threads often wait for I/O, but it will **not** speed up CPU-heavy code.\n",
        "- Multiprocessing avoids the GIL by using separate processes, so it can speed up CPU-bound work."
      ],
      "id": "eL1f2h9zVe2V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edd9b50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0edd9b50",
        "outputId": "65dab87d-7797-4fcc-fd47-dcdc8401fdd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-threaded: 0.217s\n",
            "Multi-threaded: 0.219s\n",
            "Threading speedup: 0.99x\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "def cpu_intensive_task(n: int) -> int:\n",
        "    \"\"\"Simulate CPU-bound work by doing many math operations.\"\"\"\n",
        "    total = 0\n",
        "    for i in range(n):\n",
        "        total += i ** 2\n",
        "    return total\n",
        "\n",
        "\n",
        "# Single-threaded execution\n",
        "start = time.perf_counter()\n",
        "result1 = cpu_intensive_task(1_000_000)\n",
        "result2 = cpu_intensive_task(1_000_000)\n",
        "single_duration = time.perf_counter() - start\n",
        "print(f\"Single-threaded: {single_duration:.3f}s\")\n",
        "\n",
        "# Multi-threaded execution (the GIL prevents a speedup for CPU-bound work)\n",
        "start = time.perf_counter()\n",
        "thread1 = threading.Thread(target=cpu_intensive_task, args=(1_000_000,))\n",
        "thread2 = threading.Thread(target=cpu_intensive_task, args=(1_000_000,))\n",
        "thread1.start()\n",
        "thread2.start()\n",
        "thread1.join()\n",
        "thread2.join()\n",
        "threaded_duration = time.perf_counter() - start\n",
        "print(f\"Multi-threaded: {threaded_duration:.3f}s\")\n",
        "print(f\"Threading speedup: {single_duration / threaded_duration:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb8e3f5",
      "metadata": {
        "id": "2fb8e3f5"
      },
      "source": [
        "**What to notice:** even with two threads, the CPU-heavy loop does not get faster because the GIL lets only one thread execute Python bytecode at a time. Use multiprocessing for CPU-bound work instead."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b09ff09",
      "metadata": {
        "id": "1b09ff09"
      },
      "source": [
        "## Threading Basics\n",
        "\n",
        "Threading runs multiple threads inside one process. Threads share memory, which makes sharing data easy but requires synchronization when writing to shared objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f176f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f176f9",
        "outputId": "fa71b869-fcb2-45be-8625-7032f241cf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sequential Downloads ===\n",
            "Starting download 0\n",
            "Finished download 0\n",
            "Starting download 1\n",
            "Finished download 1\n",
            "Starting download 2\n",
            "Finished download 2\n",
            "Starting download 3\n",
            "Finished download 3\n",
            "Sequential time: 2.00s\n",
            "=== Concurrent Downloads (Threading) ===\n",
            "Starting download 0\n",
            "Starting download 1\n",
            "Starting download 2\n",
            "Starting download 3\n",
            "Finished download 0\n",
            "Finished download 2\n",
            "Finished download 3\n",
            "Finished download 1\n",
            "Threaded time: 0.50s\n",
            "Speedup: 3.98x\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "def download_file(file_id: int) -> str:\n",
        "    \"\"\"Pretend to download a file; here we just sleep to simulate network delay.\"\"\"\n",
        "    print(f\"Starting download {file_id}\")\n",
        "    time.sleep(0.5)  # Simulate network latency\n",
        "    print(f\"Finished download {file_id}\")\n",
        "    return f\"data_{file_id}\"\n",
        "\n",
        "\n",
        "print(\"=== Sequential Downloads ===\")\n",
        "start = time.perf_counter()\n",
        "for i in range(4):\n",
        "    download_file(i)\n",
        "sequential_duration = time.perf_counter() - start\n",
        "print(f\"Sequential time: {sequential_duration:.2f}s\")\n",
        "\n",
        "print(\"=== Concurrent Downloads (Threading) ===\")\n",
        "start = time.perf_counter()\n",
        "threads = []\n",
        "for i in range(4):\n",
        "    thread = threading.Thread(target=download_file, args=(i,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for every thread to finish before measuring time\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "threaded_duration = time.perf_counter() - start\n",
        "print(f\"Threaded time: {threaded_duration:.2f}s\")\n",
        "print(f\"Speedup: {sequential_duration / threaded_duration:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8405d56b",
      "metadata": {
        "id": "8405d56b"
      },
      "source": [
        "**Quiz:** When is threading most beneficial?\n",
        "\n",
        "- [ ] CPU-intensive math computations\n",
        "- [x] Downloading many files from the internet\n",
        "- [ ] Sorting huge arrays entirely in memory\n",
        "- [ ] Computing prime numbers\n",
        "\n",
        "Threading shines when tasks mostly wait for I/O."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b912380",
      "metadata": {
        "id": "0b912380"
      },
      "source": [
        "## Thread Safety and Locks\n",
        "\n",
        "When threads update shared data, use locks to avoid race conditions. A `Lock` ensures only one thread enters a protected block at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac18260b",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac18260b",
        "outputId": "265f3830-b1da-448a-bd13-e5220ed5c5a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bank Account Simulation with Thread Safety ===\n",
            "Initial balance: $1000.00\n",
            "\n",
            "Alice depositing $79.73\n",
            "Alice deposited $79.73. New balance: $1079.73\n",
            "Bob depositing $17.47\n",
            "Bob deposited $17.47. New balance: $1097.20\n",
            "Charlie depositing $86.10\n",
            "Charlie deposited $86.10. New balance: $1183.31\n",
            "Alice depositing $61.66\n",
            "Alice deposited $61.66. New balance: $1244.97\n",
            "Bob depositing $57.70\n",
            "Bob deposited $57.70. New balance: $1302.67\n",
            "Charlie depositing $67.43\n",
            "Charlie deposited $67.43. New balance: $1370.10\n",
            "Alice depositing $43.19\n",
            "Alice deposited $43.19. New balance: $1413.29\n",
            "Charlie depositing $48.25\n",
            "Charlie deposited $48.25. New balance: $1461.54\n",
            "Bob depositing $83.83\n",
            "Bob deposited $83.83. New balance: $1545.37\n",
            "Alice attempting to withdraw $92.39\n",
            "Alice withdrew $92.39. New balance: $1452.98\n",
            "Charlie attempting to withdraw $35.20\n",
            "Charlie withdrew $35.20. New balance: $1417.78\n",
            "Alice attempting to withdraw $28.16\n",
            "Alice withdrew $28.16. New balance: $1389.63\n",
            "Bob depositing $65.77\n",
            "Bob deposited $65.77. New balance: $1455.40\n",
            "Charlie attempting to withdraw $37.18\n",
            "Charlie withdrew $37.18. New balance: $1418.22\n",
            "Bob depositing $74.60\n",
            "Bob deposited $74.60. New balance: $1492.82\n",
            "\n",
            "=== Final Results ===\n",
            "Final balance: $1492.82\n",
            "Total transactions: 15\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "class BankAccount:\n",
        "    \"\"\"Simulates a bank account with thread-safe operations.\"\"\"\n",
        "\n",
        "    def __init__(self, initial_balance: float = 1000.0):\n",
        "        self.balance = initial_balance\n",
        "        self.lock = threading.Lock()\n",
        "        self.transaction_count = 0\n",
        "\n",
        "    def deposit(self, amount: float, account_holder: str):\n",
        "        \"\"\"Safely deposit money into the account.\"\"\"\n",
        "        with self.lock:\n",
        "            print(f\"{account_holder} depositing ${amount:.2f}\")\n",
        "            current_balance = self.balance\n",
        "            time.sleep(0.01)  # Simulate processing time\n",
        "            self.balance = current_balance + amount\n",
        "            self.transaction_count += 1\n",
        "            print(f\"{account_holder} deposited ${amount:.2f}. New balance: ${self.balance:.2f}\")\n",
        "\n",
        "    def withdraw(self, amount: float, account_holder: str) -> bool:\n",
        "        \"\"\"Safely withdraw money from the account.\"\"\"\n",
        "        with self.lock:\n",
        "            print(f\"{account_holder} attempting to withdraw ${amount:.2f}\")\n",
        "            if self.balance >= amount:\n",
        "                current_balance = self.balance\n",
        "                time.sleep(0.01)  # Simulate processing time\n",
        "                self.balance = current_balance - amount\n",
        "                self.transaction_count += 1\n",
        "                print(f\"{account_holder} withdrew ${amount:.2f}. New balance: ${self.balance:.2f}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"{account_holder} withdrawal failed - insufficient funds\")\n",
        "                return False\n",
        "\n",
        "    def get_balance(self) -> float:\n",
        "        \"\"\"Thread-safe balance inquiry.\"\"\"\n",
        "        with self.lock:\n",
        "            return self.balance\n",
        "\n",
        "\n",
        "def perform_transactions(account: BankAccount, person_name: str, num_transactions: int):\n",
        "    \"\"\"Perform random deposits and withdrawals.\"\"\"\n",
        "    for _ in range(num_transactions):\n",
        "        action = random.choice(['deposit', 'withdraw'])\n",
        "        amount = random.uniform(10, 100)\n",
        "\n",
        "        if action == 'deposit':\n",
        "            account.deposit(amount, person_name)\n",
        "        else:\n",
        "            account.withdraw(amount, person_name)\n",
        "\n",
        "        time.sleep(random.uniform(0.01, 0.05))\n",
        "\n",
        "\n",
        "# Create a shared bank account\n",
        "print(\"=== Bank Account Simulation with Thread Safety ===\")\n",
        "account = BankAccount(initial_balance=1000.0)\n",
        "print(f\"Initial balance: ${account.get_balance():.2f}\\n\")\n",
        "\n",
        "# Multiple people accessing the same account concurrently\n",
        "people = [\"Alice\", \"Bob\", \"Charlie\"]\n",
        "transaction_threads = []\n",
        "\n",
        "for person in people:\n",
        "    thread = threading.Thread(target=perform_transactions, args=(account, person, 5))\n",
        "    transaction_threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all transactions to complete\n",
        "for thread in transaction_threads:\n",
        "    thread.join()\n",
        "\n",
        "print(f\"\\n=== Final Results ===\")\n",
        "print(f\"Final balance: ${account.get_balance():.2f}\")\n",
        "print(f\"Total transactions: {account.transaction_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16371cbb",
      "metadata": {
        "id": "16371cbb"
      },
      "source": [
        "## Thread Safety and Locks Exercise:\n",
        "Try the same code without Lock and check the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35921e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35921e80",
        "outputId": "164988f0-1af9-43e2-af7e-401285727dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsafe counter: 1000\n",
            "Safe counter: 5000\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "class UnsafeCounter:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def increment(self):\n",
        "        # No lock: two threads can read the same value and overwrite each other\n",
        "        current = self.count\n",
        "        current += 1\n",
        "        time.sleep(0.01)\n",
        "        self.count = current\n",
        "\n",
        "\n",
        "class SafeCounter:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def increment(self):\n",
        "        # Lock prevents overlapping writes\n",
        "        with self.lock:\n",
        "            current = self.count\n",
        "            current += 1\n",
        "            time.sleep(0.01)\n",
        "            self.count = current\n",
        "\n",
        "\n",
        "def worker(counter, iterations: int):\n",
        "    for _ in range(iterations):\n",
        "        counter.increment()\n",
        "\n",
        "\n",
        "# Unsafe counter test\n",
        "unsafe = UnsafeCounter()\n",
        "threads = [threading.Thread(target=worker, args=(unsafe, 1000)) for _ in range(5)]\n",
        "for t in threads:\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "print(f\"Unsafe counter: {unsafe.count}\")\n",
        "\n",
        "# Safe counter test\n",
        "safe = SafeCounter()\n",
        "threads = [threading.Thread(target=worker, args=(safe, 1000)) for _ in range(5)]\n",
        "for t in threads:\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "print(f\"Safe counter: {safe.count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59733a03",
      "metadata": {
        "id": "59733a03"
      },
      "source": [
        "Locks add overhead but prevent data races. Use them around writes to shared mutable state, or prefer thread-safe data structures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3a0009",
      "metadata": {
        "id": "1e3a0009"
      },
      "source": [
        "## ThreadPoolExecutor for Easier Threading\n",
        "\n",
        "`concurrent.futures.ThreadPoolExecutor` manages a pool of worker threads for you. Submit callables and collect results as they finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8cbc15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f8cbc15",
        "outputId": "2c10b727-7394-4ebd-8994-522830228526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Processing with ThreadPoolExecutor ===\n",
            "Processed item 0: 0\n",
            "Processed item 1: 2\n",
            "Processed item 2: 4\n",
            "Processed item 3: 6\n",
            "Processed item 4: 8\n",
            "Processed item 6: 12\n",
            "Processed item 7: 14\n",
            "Processed item 5: 10\n",
            "Processed item 8: 16\n",
            "Processed item 9: 18\n",
            "Total time: 0.61s\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "\n",
        "def process_data(item_id: int):\n",
        "    \"\"\"Simulate processing a data item that is I/O-bound.\"\"\"\n",
        "    time.sleep(0.2)\n",
        "    return f\"Processed item {item_id}\", item_id * 2\n",
        "\n",
        "\n",
        "print(\"=== Processing with ThreadPoolExecutor ===\")\n",
        "start = time.perf_counter()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(process_data, i) for i in range(10)]\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "        message, result = future.result()\n",
        "        print(f\"{message}: {result}\")\n",
        "\n",
        "duration = time.perf_counter() - start\n",
        "print(f\"Total time: {duration:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18852f19",
      "metadata": {
        "id": "18852f19"
      },
      "source": [
        "`ThreadPoolExecutor` is ideal when you want simple parallel I/O without manually creating and tracking threads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f9f630",
      "metadata": {
        "id": "09f9f630"
      },
      "source": [
        "## Multiprocessing for CPU-Bound Tasks\n",
        "\n",
        "Use the `multiprocessing` module to run CPU-heavy code in separate processes. Each process has its own Python interpreter, so the GIL is not a bottleneck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6cb35a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6cb35a",
        "outputId": "8ce4fc50-7b01-41b0-f76a-dfc5fe670f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Process ID: 422\n",
            "Starting multiprocessing pool...\n",
            "Task 1000000 started on Process ID: 26330Task 2000000 started on Process ID: 26331\n",
            "\n",
            "Task 3000000 started on Process ID: 26330\n",
            "Task 4000000 started on Process ID: 26331\n",
            "\n",
            "All tasks completed. Results:\n",
            "Task 1000000 result: 333332833333500000 (Processed by PID 26330)\n",
            "Task 2000000 result: 2666664666667000000 (Processed by PID 26331)\n",
            "Task 3000000 result: 8999995500000500000 (Processed by PID 26330)\n",
            "Task 4000000 result: 21333325333334000000 (Processed by PID 26331)\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "\n",
        "def cpu_heavy_task(x):\n",
        "    \"\"\"\n",
        "    A CPU-intensive task that calculates the sum of squares up to x.\n",
        "    It prints the Process ID (PID) to show which process is executing it.\n",
        "    \"\"\"\n",
        "    pid = os.getpid()\n",
        "    print(f\"Task {x} started on Process ID: {pid}\")\n",
        "\n",
        "    # Simulate heavy computation\n",
        "    result = sum(i * i for i in range(x))\n",
        "\n",
        "    return f\"Task {x} result: {result} (Processed by PID {pid})\"\n",
        "\n",
        "# Prepare a list of inputs for the heavy task\n",
        "inputs = [1000000, 2000000, 3000000, 4000000]\n",
        "\n",
        "print(f\"Main Process ID: {os.getpid()}\")\n",
        "print(\"Starting multiprocessing pool...\")\n",
        "\n",
        "# Create a Pool of worker processes.\n",
        "# By default, it creates one worker per CPU core available.\n",
        "# Since these are separate processes, each has its own memory and Python interpreter (GIL).\n",
        "with multiprocessing.Pool() as pool:\n",
        "    # pool.map distributes the inputs across the worker processes\n",
        "    # and collects the results in a list.\n",
        "    results = pool.map(cpu_heavy_task, inputs)\n",
        "\n",
        "print(\"\\nAll tasks completed. Results:\")\n",
        "for res in results:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the below code in VS Code. And think why Colab is not a good place to run this code?"
      ],
      "metadata": {
        "id": "ylol1jMNfelW"
      },
      "id": "ylol1jMNfelW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415b3004",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "415b3004",
        "outputId": "cc667597-17cf-4afb-e8f1-153475e0590b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Process ID: 422\n",
            "Starting multiprocessing pool...\n",
            "Task 2000000 started on Process ID: 26360Task 1000000 started on Process ID: 26359\n",
            "\n",
            "Task 3000000 started on Process ID: 26359\n",
            "Task 4000000 started on Process ID: 26360\n",
            "\n",
            "All tasks completed. Results:\n",
            "Task 1000000 result: 333332833333500000 (Processed by PID 26359)\n",
            "Task 2000000 result: 2666664666667000000 (Processed by PID 26360)\n",
            "Task 3000000 result: 8999995500000500000 (Processed by PID 26359)\n",
            "Task 4000000 result: 21333325333334000000 (Processed by PID 26360)\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "\n",
        "def cpu_heavy_task(x):\n",
        "    \"\"\"\n",
        "    A CPU-intensive task that calculates the sum of squares up to x.\n",
        "    It prints the Process ID (PID) to show which process is executing it.\n",
        "    \"\"\"\n",
        "    pid = os.getpid()\n",
        "    print(f\"Task {x} started on Process ID: {pid}\")\n",
        "\n",
        "    # Simulate heavy computation\n",
        "    result = sum(i * i for i in range(x))\n",
        "\n",
        "    return f\"Task {x} result: {result} (Processed by PID {pid})\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Prepare a list of inputs for the heavy task\n",
        "    inputs = [1000000, 2000000, 3000000, 4000000]\n",
        "    print(f\"Main Process ID: {os.getpid()}\")\n",
        "    print(\"Starting multiprocessing pool...\")\n",
        "\n",
        "    # Create a Pool of worker processes.\n",
        "    # By default, it creates one worker per CPU core available.\n",
        "    # Since these are separate processes, each has its own memory and Python interpreter (GIL).\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        # pool.map distributes the inputs across the worker processes\n",
        "        # and collects the results in a list.\n",
        "        results = pool.map(cpu_heavy_task, inputs)\n",
        "\n",
        "    print(\"\\nAll tasks completed. Results:\")\n",
        "    for res in results:\n",
        "        print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF-cJp98Ve2a"
      },
      "source": [
        "Multiprocessing can approach linear speedup for CPU work when you have enough cores. Remember that processes do not share memory."
      ],
      "id": "WF-cJp98Ve2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp5oMco8Ve2a"
      },
      "source": [
        "## Process Communication with Queues\n",
        "\n",
        "Because processes do not share memory, you need explicit communication. `multiprocessing.Queue` provides safe, cross-process message passing."
      ],
      "id": "Vp5oMco8Ve2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18ihrDWTVe2a",
        "outputId": "c5154c1c-6ca4-4478-d2f9-565b01472c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producing data_1\n",
            "Consuming data_1\n",
            "Producing data_2\n",
            "Consuming data_2\n",
            "Producing data_3\n",
            "Producing data_4\n",
            "Consuming data_3\n",
            "Consuming data_4\n",
            "All processing complete\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import time\n",
        "\n",
        "\n",
        "def producer(queue, items):\n",
        "    \"\"\"Produce items and put them on the queue.\"\"\"\n",
        "    for item in items:\n",
        "        print(f\"Producing {item}\")\n",
        "        queue.put(item)\n",
        "        time.sleep(0.1)\n",
        "    queue.put(None)  # Sentinel value to signal completion\n",
        "\n",
        "\n",
        "def consumer(queue):\n",
        "    \"\"\"Consume items until the sentinel arrives.\"\"\"\n",
        "    while True:\n",
        "        item = queue.get()\n",
        "        if item is None:\n",
        "            break\n",
        "        print(f\"Consuming {item}\")\n",
        "        time.sleep(0.15)\n",
        "\n",
        "\n",
        "queue = multiprocessing.Queue()\n",
        "items = [\"data_1\", \"data_2\", \"data_3\", \"data_4\"]\n",
        "\n",
        "producer_process = multiprocessing.Process(target=producer, args=(queue, items))\n",
        "consumer_process = multiprocessing.Process(target=consumer, args=(queue,))\n",
        "\n",
        "producer_process.start()\n",
        "consumer_process.start()\n",
        "\n",
        "producer_process.join()\n",
        "consumer_process.join()\n",
        "\n",
        "print(\"All processing complete\")\n"
      ],
      "id": "18ihrDWTVe2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGzLLzkBVe2a"
      },
      "source": [
        "Queues make it easy to build producer-consumer pipelines without sharing mutable state directly."
      ],
      "id": "BGzLLzkBVe2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH0KIiOOVe2a"
      },
      "source": [
        "## Introduction to Asyncio\n",
        "\n",
        "Asyncio lets one thread juggle many I/O-bound tasks by switching when tasks await I/O. It is ideal for thousands of network operations."
      ],
      "id": "rH0KIiOOVe2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTeWfz3-Ve2a",
        "outputId": "192c0591-ac96-45fd-99f8-7eb03e7cb1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching from source 1...\n",
            "Fetching from source 2...\n",
            "Fetching from source 3...\n",
            "Fetching from source 4...\n",
            "Received data from source 4\n",
            "Received data from source 2\n",
            "Received data from source 3\n",
            "Received data from source 1\n",
            "Results: ['data_1', 'data_2', 'data_3', 'data_4']\n",
            "Total time: 0.50s (close to the longest delay, not the sum)\n"
          ]
        }
      ],
      "source": [
        "import asyncio   # Built-in library for asynchronous (non-blocking) programming\n",
        "import time      # Used here just to measure elapsed time\n",
        "\n",
        "\n",
        "# Define an *asynchronous* function (a coroutine) using `async def`\n",
        "async def fetch_data(source_id: int, delay: float):\n",
        "    \"\"\"Simulate fetching data from a remote source.\"\"\"\n",
        "    # This prints immediately when the coroutine starts running\n",
        "    print(f\"Fetching from source {source_id}...\")\n",
        "\n",
        "    # Non-blocking sleep: while this task is \"waiting\", other tasks can run\n",
        "    # In async code we use `await asyncio.sleep(...)` instead of `time.sleep(...)`\n",
        "    await asyncio.sleep(delay)\n",
        "\n",
        "    # This prints after the delay has passed\n",
        "    print(f\"Received data from source {source_id}\")\n",
        "\n",
        "    # Return some fake \"data\" for this source\n",
        "    return f\"data_{source_id}\"\n",
        "\n",
        "\n",
        "# Another coroutine that coordinates everything\n",
        "async def main():\n",
        "    # Take a high-precision timestamp so we can measure total runtime\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    # Create a list of coroutine objects for four different \"fetch\" operations.\n",
        "    # Note: these are NOT running yet; they are just defined tasks.\n",
        "    tasks = [\n",
        "        fetch_data(1, 0.5),  # Source 1 will \"take\" 0.5 seconds\n",
        "        fetch_data(2, 0.3),  # Source 2 will \"take\" 0.3 seconds\n",
        "        fetch_data(3, 0.4),  # Source 3 will \"take\" 0.4 seconds\n",
        "        fetch_data(4, 0.2),  # Source 4 will \"take\" 0.2 seconds\n",
        "    ]\n",
        "\n",
        "    # `asyncio.gather` runs all the tasks *concurrently*.\n",
        "    # `await` waits until they are all finished and then returns their results.\n",
        "    # The results are returned in the same order as `tasks`, not by completion time.\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Work out how long everything took in total\n",
        "    duration = time.perf_counter() - start\n",
        "\n",
        "    # Show the collected results (a list of strings like \"data_1\", \"data_2\", ...)\n",
        "    print(f\"Results: {results}\")\n",
        "\n",
        "    # Show total time. Because tasks run concurrently, this should be close to\n",
        "    # the *longest* individual delay (0.5s), not the sum of all delays.\n",
        "    print(f\"Total time: {duration:.2f}s (close to the longest delay, not the sum)\")\n",
        "\n",
        "\n",
        "# --- NOTE FOR COLAB / JUPYTER ---\n",
        "# In Google Colab and Jupyter, an event loop is already running in the background.\n",
        "# That means `asyncio.run(main())` will raise an error.\n",
        "# Instead, we can directly `await main()` at the top level of a cell.\n",
        "# This line must be run in a cell by itself (or at the bottom of the cell) in Colab.\n",
        "await main()\n",
        "# --- NOTE FOR VS CODE ---\n",
        "#Use the below line instead of the above line to run this code in VS CODE:\n",
        "#asyncio.run(main())"
      ],
      "id": "oTeWfz3-Ve2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8sk8OnAVe2a"
      },
      "source": [
        "Asyncio uses cooperative multitasking: tasks run until they `await` something slow, then another task runs. Great for sockets, HTTP calls, and database queries.\n",
        "\n",
        "**Quiz:** What is the main advantage of asyncio over threading?\n",
        "\n",
        "- [ ] Better for CPU-bound tasks\n",
        "- [x] Lower overhead for managing many concurrent I/O operations\n",
        "- [ ] Automatically uses multiple CPU cores\n",
        "- [ ] Simpler syntax than threads"
      ],
      "id": "w8sk8OnAVe2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xy80ygeVe2a"
      },
      "source": [
        "## Async/Await Syntax\n",
        "\n",
        "Use `async def` to declare coroutines and `await` to pause them. `asyncio.gather` runs many coroutines at once and collects their results."
      ],
      "id": "1xy80ygeVe2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3rZzx4_Ve2a",
        "outputId": "0958bdb5-02d3-4a9b-f4df-abbe76a68603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download: https://warwick.ac.uk/\n",
            "Starting download: https://warwick.ac.uk/fac/sci/wmg/study/masters-degrees/applied-artificial-intelligence/\n",
            "Starting download: https://warwick.ac.uk/fac/sci/wmg/\n",
            "Completed download: https://warwick.ac.uk/ -> downloads/warwick.html\n",
            "Completed download: https://warwick.ac.uk/fac/sci/wmg/study/masters-degrees/applied-artificial-intelligence/ -> downloads/applied_ai.html\n",
            "Completed download: https://warwick.ac.uk/fac/sci/wmg/ -> downloads/wmg.html\n",
            "Downloaded 72931 bytes from https://warwick.ac.uk/ to downloads/warwick.html\n",
            "Downloaded 78613 bytes from https://warwick.ac.uk/fac/sci/wmg/study/masters-degrees/applied-artificial-intelligence/ to downloads/applied_ai.html\n",
            "Downloaded 133110 bytes from https://warwick.ac.uk/fac/sci/wmg/ to downloads/wmg.html\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from pathlib import Path\n",
        "\n",
        "import aiohttp\n",
        "# aiohttp is an async HTTP client library.\n",
        "# It integrates nicely with asyncio, allowing us to make non-blocking HTTP requests.\n",
        "\n",
        "\n",
        "async def download_file(session: aiohttp.ClientSession, url: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a page and save it to a file.\n",
        "\n",
        "    - `session` is reused across requests for efficiency (connection pooling).\n",
        "    - `url` is the address of the page to download.\n",
        "    - `filename` is where we’ll save the content locally.\n",
        "    \"\"\"\n",
        "    print(f\"Starting download: {url}\")\n",
        "\n",
        "    # `async with` ensures the request is properly closed when we're done,\n",
        "    # even if an exception happens. This is important for freeing resources.\n",
        "    async with session.get(url) as response:\n",
        "        # `.raise_for_status()` will raise an exception for HTTP errors (4xx/5xx),\n",
        "        # making debugging easier instead of silently failing.\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # `await response.read()` is non-blocking; it lets other tasks run while\n",
        "        # waiting for the network data to arrive.\n",
        "        content = await response.read()\n",
        "\n",
        "    # Use `pathlib.Path` instead of raw strings for safer, clearer path handling.\n",
        "    downloads_dir = Path(\"downloads\")\n",
        "    # `exist_ok=True` stops it raising an error if the directory already exists.\n",
        "    downloads_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    filepath = downloads_dir / filename\n",
        "    # `.write_bytes()` writes the raw bytes to disk in one go (no need to open/close manually).\n",
        "    filepath.write_bytes(content)\n",
        "\n",
        "    print(f\"Completed download: {url} -> {filepath}\")\n",
        "    return f\"Downloaded {len(content)} bytes from {url} to {filepath}\"\n",
        "\n",
        "\n",
        "async def download_all_files():\n",
        "    \"\"\"\n",
        "    Create and run all download tasks concurrently.\n",
        "\n",
        "    This function:\n",
        "    - Defines which URLs to download.\n",
        "    - Opens a single shared HTTP session (for efficiency).\n",
        "    - Starts all downloads at once using `asyncio.gather`.\n",
        "    \"\"\"\n",
        "    files = [\n",
        "        (\"https://warwick.ac.uk/\", \"warwick.html\"),\n",
        "        (\n",
        "            \"https://warwick.ac.uk/fac/sci/wmg/study/masters-degrees/applied-artificial-intelligence/\",\n",
        "            \"applied_ai.html\",\n",
        "        ),\n",
        "        (\"https://warwick.ac.uk/fac/sci/wmg/\", \"wmg.html\"),\n",
        "    ]\n",
        "\n",
        "    # Use one ClientSession for multiple requests:\n",
        "    # - Reuses TCP connections (connection pooling)\n",
        "    # - Faster and more resource-efficient than creating a new session per request.\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # Build a list of coroutine objects (not yet executed):\n",
        "        tasks = [\n",
        "            download_file(session, url, filename)\n",
        "            for url, filename in files\n",
        "        ]\n",
        "\n",
        "        # `asyncio.gather(*tasks)` schedules all the tasks to run concurrently\n",
        "        # and waits for all of them to finish.\n",
        "        #\n",
        "        # This is where we get the benefit of async: while one download is waiting\n",
        "        # on network I/O, another one can progress.\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Return the list of result strings from all the downloads.\n",
        "    return results\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Top-level async entry point.\n",
        "\n",
        "    This function orchestrates the whole workflow:\n",
        "    - Calls `download_all_files` to perform all network work.\n",
        "    - Prints the results returned for each download.\n",
        "    \"\"\"\n",
        "    results = await download_all_files()\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "\n",
        "# In a normal Python script (run from the terminal), you’d typically do:\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(main())\n",
        "#\n",
        "# But in Jupyter / IPython, the event loop is already running,\n",
        "# so we can directly `await main()` instead:\n",
        "await main()\n"
      ],
      "id": "r3rZzx4_Ve2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPAZMLzVe2a"
      },
      "source": [
        "`await` hands control back to the event loop so other tasks can run while one is waiting on I/O."
      ],
      "id": "YtPAZMLzVe2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAGT8WFqVe2a"
      },
      "source": [
        "Async context managers guarantee cleanup (even on errors), and async iterators make it easy to work with streaming data without blocking the loop."
      ],
      "id": "CAGT8WFqVe2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjdQ4cUVe2b"
      },
      "source": [
        "## Choosing the Right Approach\n",
        "\n",
        "Use this quick guide:\n",
        "\n",
        "| Task type | Best approach | Why |\n",
        "| --- | --- | --- |\n",
        "| I/O-bound (few connections) | Threading | Simple and low overhead |\n",
        "| I/O-bound (many connections) | Asyncio | Scales to thousands with low overhead |\n",
        "| CPU-bound | Multiprocessing | Uses multiple cores and skips the GIL |\n",
        "| Mixed I/O + CPU | Asyncio plus process pool | Async for I/O, processes for heavy work |"
      ],
      "id": "qVjdQ4cUVe2b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Right Concurrency Model\n",
        "\n",
        "Below is a quick reference for why each method is a good fit for the given example.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Web scraping 200 URLs → `asyncio`\n",
        "\n",
        "**Best because:**\n",
        "\n",
        "- **I/O-bound workload**  \n",
        "  - Web scraping spends most of its time **waiting for network responses**, not using much CPU.\n",
        "- **High concurrency needed (200+ tasks)**  \n",
        "  - Doing 200 requests sequentially is slow; you want to **overlap the waiting time**.\n",
        "- **`asyncio` scales well to many tasks**  \n",
        "  - Runs everything in **one thread, one process**.\n",
        "  - While one request is waiting (`await`), the event loop runs other coroutines.\n",
        "  - Coroutines are **lighter than threads**, so hundreds or thousands of concurrent tasks are practical.\n",
        "- **Threads are possible but heavier**  \n",
        "  - 200 threads use more memory and incur more context-switching overhead.\n",
        "  - `asyncio` is designed specifically for “lots of I/O, lots of tasks”.\n",
        "\n",
        "> **Summary:** Many network-bound tasks (200 URLs) → `asyncio` gives efficient, scalable concurrency.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Image processing 50 files → `multiprocessing`\n",
        "\n",
        "**Best because:**\n",
        "\n",
        "- **CPU-bound workload**  \n",
        "  - Image decoding, resizing, filtering, and transforms are **computationally heavy**.\n",
        "- **The GIL limits threads for CPU work**  \n",
        "  - In CPython, the **Global Interpreter Lock (GIL)** means only one thread executes Python bytecode at a time **per process**.\n",
        "  - Threads do **not** give true parallelism for CPU-heavy pure Python code.\n",
        "- **`multiprocessing` uses multiple processes**  \n",
        "  - Each process has its own interpreter and its own GIL.\n",
        "  - Multiple processes can run on **different CPU cores in parallel**.\n",
        "- **Good match for 50 images**  \n",
        "  - Use a process pool (e.g. 4–8 workers).\n",
        "  - Each process handles images one by one, fully using multiple cores.\n",
        "- **Threads here would underuse the CPU**  \n",
        "  - They would still contend on the GIL and not speed things up much.\n",
        "\n",
        "> **Summary:** Heavy CPU-bound work (image processing) → `multiprocessing` bypasses the GIL and uses multiple cores.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Database queries (10) → `threading`\n",
        "\n",
        "**Best because:**\n",
        "\n",
        "- **I/O-bound workload**  \n",
        "  - Database queries spend most time **waiting on the database server** to respond.\n",
        "- **Only a small number of concurrent tasks (10)**  \n",
        "  - Overhead of 10 threads is tiny; we do not need the massive scalability of `asyncio`.\n",
        "- **Typical DB drivers are blocking and thread-safe**  \n",
        "  - Most database libraries expose a **blocking** API: `cursor.execute(query)` blocks until the result arrives.\n",
        "  - They are usually designed to work well with threads.\n",
        "- **Threads are easy to use here**  \n",
        "  - Each thread can run a query independently.\n",
        "  - The main thread waits for all threads to finish.\n",
        "  - No need to convert codebase to async or manage an event loop.\n",
        "- **`multiprocessing` is overkill**  \n",
        "  - Processes are heavier (more memory, more coordination).\n",
        "  - For 10 I/O-bound operations, the extra complexity is not worth it.\n",
        "\n",
        "> **Summary:** A small number of blocking I/O tasks (10 DB queries) → `threading` is simple, effective, and works well with typical database drivers."
      ],
      "metadata": {
        "id": "Fa-pZ6aMybFd"
      },
      "id": "Fa-pZ6aMybFd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uh7dIxhVe2b"
      },
      "source": [
        "`loop.run_in_executor` is the bridge that lets async code cooperate with blocking libraries without freezing the event loop."
      ],
      "id": "7Uh7dIxhVe2b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example:\n",
        "Try the below example using both CPU and GPU. The code automatically choose GPU when it is available."
      ],
      "metadata": {
        "id": "6Lj1n7S41kIZ"
      },
      "id": "6Lj1n7S41kIZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# 1. Install dependencies\n",
        "# =======================\n",
        "!pip install -q openai-whisper aiohttp\n",
        "!apt-get -y install ffmpeg > /dev/null\n",
        "\n",
        "# =======================\n",
        "# 2. Imports & setup\n",
        "# =======================\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from pathlib import Path\n",
        "import whisper\n",
        "import torch\n",
        "\n",
        "# Folder where files will be stored\n",
        "DOWNLOAD_DIR = Path(\"downloads\")\n",
        "DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Choose device (GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Whisper model (you can change \"small\" to \"tiny\", \"base\", \"medium\", \"large\", etc.)\n",
        "model = whisper.load_model(\"small\", device=device)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Async download helpers (audio / video)\n",
        "# ==========================================\n",
        "async def download_one(session: aiohttp.ClientSession, url: str, dest_folder: Path = DOWNLOAD_DIR) -> Path:\n",
        "    \"\"\"\n",
        "    Download a single file using aiohttp and save it to dest_folder.\n",
        "    Returns the local file path.\n",
        "    \"\"\"\n",
        "    # Derive a filename from the URL\n",
        "    name_from_url = url.split(\"?\")[0].rstrip(\"/\").split(\"/\")[-1] or \"file\"\n",
        "    filepath = dest_folder / name_from_url\n",
        "\n",
        "    print(f\"Starting download: {url}\")\n",
        "    async with session.get(url) as resp:\n",
        "        resp.raise_for_status()\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            async for chunk in resp.content.iter_chunked(1024 * 64):\n",
        "                f.write(chunk)\n",
        "    print(f\"Completed download: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "async def download_all(urls):\n",
        "    \"\"\"\n",
        "    Download all URLs concurrently and return a list of local file paths.\n",
        "    \"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [download_one(session, url) for url in urls]\n",
        "        files = await asyncio.gather(*tasks)\n",
        "    return files\n",
        "\n",
        "# ===================================\n",
        "# 4. Transcription using Whisper\n",
        "# ===================================\n",
        "def transcribe_files(filepaths, language: str = None):\n",
        "    \"\"\"\n",
        "    Transcribe each file in filepaths with Whisper.\n",
        "    Saves a .txt file with the same name and returns a dict of {Path: text}.\n",
        "    \"\"\"\n",
        "    transcripts = {}\n",
        "    for path in filepaths:\n",
        "        print(f\"\\nTranscribing: {path}\")\n",
        "        # language=None lets Whisper auto-detect; set e.g. language=\"en\" to force English\n",
        "        result = model.transcribe(str(path), language=language)\n",
        "        text = result[\"text\"].strip()\n",
        "\n",
        "        transcripts[path] = text\n",
        "\n",
        "        # Save transcript next to the file\n",
        "        txt_path = path.with_suffix(path.suffix + \".txt\")\n",
        "        txt_path.write_text(text, encoding=\"utf-8\")\n",
        "        print(f\"Saved transcript to: {txt_path}\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "# ===================================\n",
        "# 5. Define URLs and run everything\n",
        "# ===================================\n",
        "\n",
        "# Real sample audio/video URLs intended for testing\n",
        "# - Short sample MP3 and MP4 from samplelib.com\n",
        "# - Sample MP3 and MP4 from an open GitHub repo\n",
        "URLS = [\n",
        "    \"https://download.samplelib.com/mp3/sample-3s.mp3\",   # short MP3 sample\n",
        "    \"https://github.com/rafaelreis-hotmart/Audio-Sample-files/raw/master/sample.mp3\",  # sample MP3\n",
        "    \"https://download.samplelib.com/mp4/sample-5s.mp4\",   # short MP4 sample\n",
        "    \"https://github.com/rafaelreis-hotmart/Audio-Sample-files/raw/master/sample.mp4\",  # sample MP4\n",
        "]\n",
        "\n",
        "async def main():\n",
        "    # Step 1: download all media files concurrently\n",
        "    downloaded_files = await download_all(URLS)\n",
        "\n",
        "    # Step 2: transcribe them one by one with Whisper\n",
        "    transcripts = transcribe_files(downloaded_files, language=None)  # or language=\"en\"\n",
        "\n",
        "    # Optional: print a short preview\n",
        "    print(\"\\n=== Transcript previews ===\")\n",
        "    for path, text in transcripts.items():\n",
        "        preview = text[:200].replace(\"\\n\", \" \")\n",
        "        print(f\"{path.name}: {preview}...\")\n",
        "    return transcripts\n",
        "\n",
        "# In Colab/Jupyter we use top-level `await` instead of asyncio.run(...)\n",
        "transcripts = await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a1bIYuY1Cty",
        "outputId": "60e2fec4-c286-4568-f5fb-76b0373713d3"
      },
      "id": "1a1bIYuY1Cty",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <coroutine object main at 0x7e13a637fd40>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a637fd40>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a637d740>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a63cc540>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a63cd140>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object download_all_files at 0x7e13a636e130>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object download_all_files at 0x7e13a636e130>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a63495d0>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object download_all_files at 0x7e13a636c150>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object main at 0x7e13a64c2b00>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen posixpath>:82: RuntimeWarning: coroutine 'main' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 461M/461M [00:01<00:00, 258MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download: https://download.samplelib.com/mp3/sample-3s.mp3\n",
            "Starting download: https://github.com/rafaelreis-hotmart/Audio-Sample-files/raw/master/sample.mp3\n",
            "Starting download: https://download.samplelib.com/mp4/sample-5s.mp4\n",
            "Starting download: https://github.com/rafaelreis-hotmart/Audio-Sample-files/raw/master/sample.mp4\n",
            "Completed download: downloads/sample-3s.mp3\n",
            "Completed download: downloads/sample.mp4\n",
            "Completed download: downloads/sample.mp3\n",
            "Completed download: downloads/sample-5s.mp4\n",
            "\n",
            "Transcribing: downloads/sample-3s.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transcript to: downloads/sample-3s.mp3.txt\n",
            "\n",
            "Transcribing: downloads/sample.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transcript to: downloads/sample.mp3.txt\n",
            "\n",
            "Transcribing: downloads/sample-5s.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transcript to: downloads/sample-5s.mp4.txt\n",
            "\n",
            "Transcribing: downloads/sample.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transcript to: downloads/sample.mp4.txt\n",
            "\n",
            "=== Transcript previews ===\n",
            "sample-3s.mp3: you...\n",
            "sample.mp3: Music...\n",
            "sample-5s.mp4: ...\n",
            "sample.mp4: you...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj2goObvVe2b"
      },
      "source": [
        "## Common Pitfalls: Race Conditions and Deadlocks\n",
        "\n",
        "Race conditions happen when threads interleave reads and writes to shared data without coordination. Deadlocks happen when locks are acquired in inconsistent order. Always use locks for shared writes and a consistent lock ordering."
      ],
      "id": "Nj2goObvVe2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzy5mUSUVe2b",
        "outputId": "a91a7a5f-6d1d-4a9d-ff0e-76c4fe50780a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsafe list (should have 1 item): [1]\n",
            "Safe list (should have 1 item): [1]\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "# Race condition example\n",
        "shared_list = []\n",
        "\n",
        "\n",
        "def unsafe_append(value: int):\n",
        "    if value not in shared_list:  # Check\n",
        "        time.sleep(0.001)  # Simulate work between check and write\n",
        "        shared_list.append(value)  # Modify (race window)\n",
        "\n",
        "\n",
        "threads = [threading.Thread(target=unsafe_append, args=(1,)) for _ in range(5)]\n",
        "for t in threads:\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(f\"Unsafe list (should have 1 item): {shared_list}\")\n",
        "\n",
        "# Safe version with lock\n",
        "shared_list_safe = []\n",
        "lock = threading.Lock()\n",
        "\n",
        "\n",
        "def safe_append(value: int):\n",
        "    with lock:\n",
        "        if value not in shared_list_safe:\n",
        "            time.sleep(0.001)\n",
        "            shared_list_safe.append(value)\n",
        "\n",
        "\n",
        "threads_safe = [threading.Thread(target=safe_append, args=(1,)) for _ in range(5)]\n",
        "for t in threads_safe:\n",
        "    t.start()\n",
        "for t in threads_safe:\n",
        "    t.join()\n",
        "\n",
        "print(f\"Safe list (should have 1 item): {shared_list_safe}\")\n"
      ],
      "id": "xzy5mUSUVe2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xQUgnXgVe2f",
        "outputId": "d0a1a7ce-5f3c-4114-8b32-46b30753bc8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1 acquired lock1\n",
            "Task1 acquired lock2\n",
            "Task2 acquired lock1\n",
            "Task2 acquired lock2\n",
            "No deadlock!\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "# Deadlock example and fix\n",
        "lock1 = threading.Lock()\n",
        "lock2 = threading.Lock()\n",
        "\n",
        "\n",
        "def task1_safe():\n",
        "    # Always acquire locks in the same order\n",
        "    with lock1:\n",
        "        print(\"Task1 acquired lock1\")\n",
        "        time.sleep(0.1)\n",
        "        with lock2:\n",
        "            print(\"Task1 acquired lock2\")\n",
        "\n",
        "\n",
        "def task2_safe():\n",
        "    # Same lock order prevents deadlock\n",
        "    with lock1:\n",
        "        print(\"Task2 acquired lock1\")\n",
        "        time.sleep(0.1)\n",
        "        with lock2:\n",
        "            print(\"Task2 acquired lock2\")\n",
        "\n",
        "\n",
        "t1 = threading.Thread(target=task1_safe)\n",
        "t2 = threading.Thread(target=task2_safe)\n",
        "t1.start(); t2.start()\n",
        "t1.join(); t2.join()\n",
        "print(\"No deadlock!\")\n"
      ],
      "id": "7xQUgnXgVe2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vHM6NBcVe2f"
      },
      "source": [
        "## Practice Projects\n",
        "\n",
        "Try these exercises to reinforce the ideas. Each block includes a reference solution you can run and modify."
      ],
      "id": "4vHM6NBcVe2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGa5d-5dVe2f"
      },
      "source": [
        "### Exercise 1: Parallel Data Processor (Multiprocessing)\n",
        "\n",
        "Goal: split a list of numbers across processes and compare sequential vs. parallel execution.\n",
        "\n",
        "Steps:\n",
        "1. Write a CPU-intensive function (prime check works well).\n",
        "2. Process numbers sequentially and time it.\n",
        "3. Process numbers with a process pool and time it.\n",
        "4. Compare the speedup."
      ],
      "id": "PGa5d-5dVe2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqJqv1TpVe2f"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def is_prime(n: int) -> bool:\n",
        "    \"\"\"Check if n is prime (expensive for large numbers).\"\"\"\n",
        "    if n < 2:\n",
        "        return False\n",
        "    if n == 2:\n",
        "        return True\n",
        "    if n % 2 == 0:\n",
        "        return False\n",
        "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def process_number(n: int):\n",
        "    return {\n",
        "        \"number\": n,\n",
        "        \"is_prime\": is_prime(n),\n",
        "        \"square\": n * n,\n",
        "        \"cube\": n * n * n,\n",
        "    }\n",
        "\n",
        "\n",
        "def sequential_processing(numbers):\n",
        "    start = time.perf_counter()\n",
        "    results = [process_number(n) for n in numbers]\n",
        "    duration = time.perf_counter() - start\n",
        "    return results, duration\n",
        "\n",
        "\n",
        "def parallel_processing(numbers, num_processes: int = 4):\n",
        "    start = time.perf_counter()\n",
        "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "        results = pool.map(process_number, numbers)\n",
        "    duration = time.perf_counter() - start\n",
        "    return results, duration\n",
        "\n",
        "\n",
        "# Test with large primes (primality check is expensive)\n",
        "test_numbers = [104729, 104743, 104759, 104761, 104773, 104779, 104789, 104801]\n",
        "\n",
        "print(\"=== Sequential Processing ===\")\n",
        "seq_results, seq_time = sequential_processing(test_numbers)\n",
        "print(f\"Time: {seq_time:.3f}s\")\n",
        "for r in seq_results[:3]:\n",
        "    print(f\"  {r['number']}: prime={r['is_prime']}\")\n",
        "\n",
        "print(\"\n",
        "=== Parallel Processing ===\")\n",
        "par_results, par_time = parallel_processing(test_numbers)\n",
        "print(f\"Time: {par_time:.3f}s\")\n",
        "for r in par_results[:3]:\n",
        "    print(f\"  {r['number']}: prime={r['is_prime']}\")\n",
        "\n",
        "print(f\"Speedup: {seq_time / par_time:.2f}x\")\n"
      ],
      "id": "pqJqv1TpVe2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rjIY0OvVe2f"
      },
      "source": [
        "### Exercise 2: Async Web Scraper (Asyncio)\n",
        "\n",
        "Goal: fetch many URLs concurrently with asyncio, handle errors, and limit concurrency."
      ],
      "id": "4rjIY0OvVe2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "3PCps3YLVe2f",
        "outputId": "d633b971-b0f4-4a87-e25c-fe41751c1e24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 50) (ipython-input-228327426.py, line 50)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-228327426.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 50)\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "\n",
        "\n",
        "class AsyncWebScraper:\n",
        "    \"\"\"Asynchronous web scraper with concurrency control.\"\"\"\n",
        "\n",
        "    def __init__(self, max_concurrent: int = 5):\n",
        "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
        "\n",
        "    async def fetch_url(self, url: str, delay: float = 0.5):\n",
        "        async with self.semaphore:\n",
        "            try:\n",
        "                print(f\"Fetching: {url}\")\n",
        "                await asyncio.sleep(delay)  # Simulate network request\n",
        "\n",
        "                if \"error\" in url:\n",
        "                    raise ConnectionError(f\"Failed to connect to {url}\")\n",
        "\n",
        "                data = f\"Content from {url}\"\n",
        "                print(f\"Completed: {url}\")\n",
        "                return {\"url\": url, \"status\": \"success\", \"data\": data}\n",
        "\n",
        "            except ConnectionError as exc:\n",
        "                print(f\"Error: {exc}\")\n",
        "                return {\"url\": url, \"status\": \"error\", \"data\": None}\n",
        "\n",
        "    async def scrape_urls(self, urls):\n",
        "        tasks = [self.fetch_url(url) for url in urls]\n",
        "        return await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"http://example.com/page1\",\n",
        "        \"http://example.com/page2\",\n",
        "        \"http://example.com/page3\",\n",
        "        \"http://example.com/error\",  # Will fail\n",
        "        \"http://example.com/page4\",\n",
        "        \"http://example.com/page5\",\n",
        "        \"http://example.com/page6\",\n",
        "    ]\n",
        "\n",
        "    scraper = AsyncWebScraper(max_concurrent=3)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    results = await scraper.scrape_urls(urls)\n",
        "    duration = time.perf_counter() - start\n",
        "\n",
        "    print(\"\n",
        "=== Results ===\")\n",
        "    successful = sum(1 for r in results if isinstance(r, dict) and r.get(\"status\") == \"success\")\n",
        "    print(f\"Successful: {successful}/{len(urls)}\")\n",
        "    print(f\"Total time: {duration:.2f}s\")\n",
        "    print(f\"Average time per URL: {duration / len(urls):.2f}s\")\n",
        "\n",
        "\n",
        "asyncio.run(main())\n"
      ],
      "id": "3PCps3YLVe2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLGbR9Q_Ve2f"
      },
      "source": [
        "Try adding retries with exponential backoff for failed URLs to make the scraper more robust."
      ],
      "id": "yLGbR9Q_Ve2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knMYtRQtVe2f"
      },
      "source": [
        "### Exercise 3: Producer-Consumer Pipeline (Threading)\n",
        "\n",
        "Goal: implement a producer-consumer pattern with a queue and multiple consumer threads."
      ],
      "id": "knMYtRQtVe2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmTRhQvAVe2f"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "class DataPipeline:\n",
        "    \"\"\"Producer-consumer pipeline with a thread-safe queue.\"\"\"\n",
        "\n",
        "    def __init__(self, queue_size: int = 10):\n",
        "        self.queue = queue.Queue(maxsize=queue_size)\n",
        "        self.stop_event = threading.Event()\n",
        "\n",
        "    def producer(self, producer_id: int, num_items: int):\n",
        "        for i in range(num_items):\n",
        "            if self.stop_event.is_set():\n",
        "                break\n",
        "\n",
        "            item = {\n",
        "                \"id\": f\"P{producer_id}-{i}\",\n",
        "                \"value\": random.randint(1, 100),\n",
        "                \"timestamp\": time.time(),\n",
        "            }\n",
        "\n",
        "            print(f\"Producer {producer_id}: Creating {item['id']}\")\n",
        "            self.queue.put(item)\n",
        "            time.sleep(random.uniform(0.1, 0.3))\n",
        "\n",
        "        print(f\"Producer {producer_id}: Finished\")\n",
        "\n",
        "    def consumer(self, consumer_id: int):\n",
        "        while not self.stop_event.is_set():\n",
        "            try:\n",
        "                item = self.queue.get(timeout=1)\n",
        "                print(f\"Consumer {consumer_id}: Processing {item['id']}\")\n",
        "                time.sleep(random.uniform(0.2, 0.5))\n",
        "                result = item['value'] * 2\n",
        "                print(f\"Consumer {consumer_id}: Completed {item['id']} -> {result}\")\n",
        "                self.queue.task_done()\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "        print(f\"Consumer {consumer_id}: Finished\")\n",
        "\n",
        "    def run(self, num_producers: int = 2, num_consumers: int = 3, items_per_producer: int = 5):\n",
        "        threads = []\n",
        "\n",
        "        # Start consumers first so they are ready to work\n",
        "        for i in range(num_consumers):\n",
        "            t = threading.Thread(target=self.consumer, args=(i,))\n",
        "            t.start()\n",
        "            threads.append(t)\n",
        "\n",
        "        # Start producers\n",
        "        for i in range(num_producers):\n",
        "            t = threading.Thread(target=self.producer, args=(i, items_per_producer))\n",
        "            t.start()\n",
        "            threads.append(t)\n",
        "\n",
        "        # Wait for all producers to finish\n",
        "        for t in threads[num_consumers:]:\n",
        "            t.join()\n",
        "\n",
        "        # Wait until queue is empty\n",
        "        self.queue.join()\n",
        "        self.stop_event.set()\n",
        "\n",
        "        # Stop consumers\n",
        "        for t in threads[:num_consumers]:\n",
        "            t.join()\n",
        "\n",
        "        print(\"\n",
        "=== Pipeline Complete ===\")\n",
        "\n",
        "\n",
        "pipeline = DataPipeline(queue_size=5)\n",
        "pipeline.run(num_producers=2, num_consumers=3, items_per_producer=4)\n"
      ],
      "id": "JmTRhQvAVe2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND96pCd9Ve2g"
      },
      "source": [
        "Try replacing the regular queue with `queue.PriorityQueue()` and use the item value as the priority so larger numbers process first."
      ],
      "id": "ND96pCd9Ve2g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3464-o2eVe2g"
      },
      "source": [
        "### Bonus: Hybrid Async + Multiprocessing\n",
        "\n",
        "Goal: combine asyncio for I/O-bound fetching with a process pool for CPU-heavy analysis."
      ],
      "id": "3464-o2eVe2g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "DcBx-oaMVe2g",
        "outputId": "26f973b9-de78-4748-aceb-0b5bc7e94fe1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 31) (ipython-input-2881882735.py, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2881882735.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 31)\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import math\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import random\n",
        "\n",
        "\n",
        "def cpu_intensive_analysis(data):\n",
        "    numbers = data.get(\"numbers\", [])\n",
        "    result = sum(math.sqrt(n) * math.log(n + 1) for n in numbers if n > 0)\n",
        "    return {\"id\": data[\"id\"], \"result\": result, \"count\": len(numbers)}\n",
        "\n",
        "\n",
        "async def fetch_dataset(dataset_id: str, delay: float = 0.3):\n",
        "    print(f\"Fetching dataset {dataset_id}...\")\n",
        "    await asyncio.sleep(delay)\n",
        "    data = {\"id\": dataset_id, \"numbers\": [random.randint(1, 1000) for _ in range(10000)]}\n",
        "    print(f\"Fetched dataset {dataset_id}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "async def process_with_hybrid_approach(dataset_ids):\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "        # Phase 1: async fetch\n",
        "        print(\"=== Phase 1: Fetching Datasets (Async I/O) ===\")\n",
        "        datasets = await asyncio.gather(*(fetch_dataset(did) for did in dataset_ids))\n",
        "\n",
        "        # Phase 2: process in separate processes\n",
        "        print(\"\n",
        "=== Phase 2: Processing Datasets (Multiprocessing) ===\")\n",
        "        process_tasks = [loop.run_in_executor(executor, cpu_intensive_analysis, ds) for ds in datasets]\n",
        "        return await asyncio.gather(*process_tasks)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    dataset_ids = [\"DS001\", \"DS002\", \"DS003\", \"DS004\"]\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    results = await process_with_hybrid_approach(dataset_ids)\n",
        "    duration = time.perf_counter() - start\n",
        "\n",
        "    print(\"\n",
        "=== Results ===\")\n",
        "    for result in results:\n",
        "        print(f\"Dataset {result['id']}: {result['result']:.2f} (processed {result['count']} numbers)\")\n",
        "\n",
        "    print(f\"\n",
        "Total time: {duration:.2f}s\")\n",
        "\n",
        "\n",
        "asyncio.run(main())\n"
      ],
      "id": "DcBx-oaMVe2g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwgjiPVbVe2g"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "- Python Threading docs: https://docs.python.org/3/library/threading.html\n",
        "- Python Multiprocessing docs: https://docs.python.org/3/library/multiprocessing.html\n",
        "- Python Asyncio docs: https://docs.python.org/3/library/asyncio.html\n",
        "- `concurrent.futures` module: https://docs.python.org/3/library/concurrent.futures.html\n",
        "- Understanding the Python GIL: https://realpython.com/python-gil/"
      ],
      "id": "cwgjiPVbVe2g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9UsuoU7Ve2g"
      },
      "source": [
        "## Recap\n",
        "\n",
        "- Threading: great for I/O-bound tasks with moderate concurrency.\n",
        "- Asyncio: best for very high I/O concurrency with low overhead.\n",
        "- Multiprocessing: best for CPU-bound tasks because it bypasses the GIL.\n",
        "- Use locks for shared state, avoid deadlocks by consistent lock ordering.\n",
        "- Mix models (async + executors) when your app has both I/O and CPU-heavy parts."
      ],
      "id": "F9UsuoU7Ve2g"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}